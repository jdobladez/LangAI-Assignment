{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, matthews_corrcoef\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Word2Vec Feature Matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "         author_ID  average_word_length  avg_sentence_length       ttr   \n0      t2_ffcfiueh             5.481442            66.164835  0.707317  \\\n1         t2_lfs48             6.285714            85.014925  0.718078   \n2         t2_zcj4y             5.580460            34.134969  0.747126   \n3      t2_2xpu7n1c             5.481297            75.441176  0.640898   \n4         t2_3edl7             5.916865            60.968085  0.719715   \n...            ...                  ...                  ...       ...   \n30934  t2_8hacr7if             5.697468            49.875000  0.683544   \n30935  t2_8hacr7if             5.790725            50.450450  0.744352   \n30936  t2_8hacr7if             5.671835            47.716981  0.700258   \n30937  t2_8hacr7if             5.748459            47.866071  0.704069   \n30938  t2_8hacr7if             5.859155            46.161290  0.892019   \n\n       nr_unique_words  nr_chars  nr_contradictions  subjectivity  nr_period   \n0                  667      6111                  6      0.485495   0.091198  \\\n1                  568      5762                 17      0.487030   0.113780   \n2                  650      5724                 18      0.551005   0.174713   \n3                  514      5197                  0      0.514366   0.073566   \n4                  606      5823                 11      0.401854   0.122328   \n...                ...       ...                ...           ...        ...   \n30934              540      5290                 13      0.437244   0.111392   \n30935              626      5710                  9      0.436489   0.109394   \n30936              542      5163                  9      0.466545   0.131783   \n30937              571      5472                  7      0.428413   0.114673   \n30938              190      1460                  6      0.425926   0.107981   \n\n       nr_comma  ...  feature_91  feature_92  feature_93  feature_94   \n0      0.047720  ...    0.030382   -0.038984    0.082427    0.027456  \\\n1      0.078382  ...   -0.002733   -0.035620    0.086639    0.089150   \n2      0.065517  ...    0.041019   -0.025408    0.076059    0.078353   \n3      0.112219  ...    0.059938   -0.004517    0.064531    0.129023   \n4      0.085511  ...   -0.016096   -0.002657    0.096512    0.061793   \n...         ...  ...         ...         ...         ...         ...   \n30934  0.037975  ...   -0.054445   -0.005667    0.073418    0.111456   \n30935  0.048751  ...   -0.053965    0.010229    0.090820    0.098168   \n30936  0.047804  ...    0.012857   -0.056423    0.078301    0.113703   \n30937  0.053021  ...    0.011493   -0.041948    0.091651    0.121716   \n30938  0.032864  ...   -0.001145    0.023824    0.112719    0.103603   \n\n       feature_95  feature_96  feature_97  feature_98  feature_99    Poles  \n0       -0.110414   -0.035993   -0.043381    0.027923   -0.085713  Western  \n1       -0.005477   -0.005393   -0.010546    0.005088   -0.034315  Western  \n2       -0.077311   -0.021945   -0.051248   -0.023123   -0.007378  Western  \n3       -0.062811    0.001734   -0.053279   -0.044899    0.018924  Western  \n4       -0.128142    0.001301   -0.059142    0.026707   -0.030807  Western  \n...           ...         ...         ...         ...         ...      ...  \n30934   -0.142758   -0.026188   -0.088024    0.020324   -0.023312  Eastern  \n30935   -0.123997    0.001711   -0.086069    0.063151   -0.029690  Eastern  \n30936   -0.119587   -0.012560   -0.153321    0.051385   -0.040174  Eastern  \n30937   -0.118058   -0.020970   -0.146114    0.043594   -0.022459  Eastern  \n30938   -0.121641   -0.011123   -0.093972    0.082738   -0.009133  Eastern  \n\n[30939 rows x 113 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author_ID</th>\n      <th>average_word_length</th>\n      <th>avg_sentence_length</th>\n      <th>ttr</th>\n      <th>nr_unique_words</th>\n      <th>nr_chars</th>\n      <th>nr_contradictions</th>\n      <th>subjectivity</th>\n      <th>nr_period</th>\n      <th>nr_comma</th>\n      <th>...</th>\n      <th>feature_91</th>\n      <th>feature_92</th>\n      <th>feature_93</th>\n      <th>feature_94</th>\n      <th>feature_95</th>\n      <th>feature_96</th>\n      <th>feature_97</th>\n      <th>feature_98</th>\n      <th>feature_99</th>\n      <th>Poles</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t2_ffcfiueh</td>\n      <td>5.481442</td>\n      <td>66.164835</td>\n      <td>0.707317</td>\n      <td>667</td>\n      <td>6111</td>\n      <td>6</td>\n      <td>0.485495</td>\n      <td>0.091198</td>\n      <td>0.047720</td>\n      <td>...</td>\n      <td>0.030382</td>\n      <td>-0.038984</td>\n      <td>0.082427</td>\n      <td>0.027456</td>\n      <td>-0.110414</td>\n      <td>-0.035993</td>\n      <td>-0.043381</td>\n      <td>0.027923</td>\n      <td>-0.085713</td>\n      <td>Western</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t2_lfs48</td>\n      <td>6.285714</td>\n      <td>85.014925</td>\n      <td>0.718078</td>\n      <td>568</td>\n      <td>5762</td>\n      <td>17</td>\n      <td>0.487030</td>\n      <td>0.113780</td>\n      <td>0.078382</td>\n      <td>...</td>\n      <td>-0.002733</td>\n      <td>-0.035620</td>\n      <td>0.086639</td>\n      <td>0.089150</td>\n      <td>-0.005477</td>\n      <td>-0.005393</td>\n      <td>-0.010546</td>\n      <td>0.005088</td>\n      <td>-0.034315</td>\n      <td>Western</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t2_zcj4y</td>\n      <td>5.580460</td>\n      <td>34.134969</td>\n      <td>0.747126</td>\n      <td>650</td>\n      <td>5724</td>\n      <td>18</td>\n      <td>0.551005</td>\n      <td>0.174713</td>\n      <td>0.065517</td>\n      <td>...</td>\n      <td>0.041019</td>\n      <td>-0.025408</td>\n      <td>0.076059</td>\n      <td>0.078353</td>\n      <td>-0.077311</td>\n      <td>-0.021945</td>\n      <td>-0.051248</td>\n      <td>-0.023123</td>\n      <td>-0.007378</td>\n      <td>Western</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t2_2xpu7n1c</td>\n      <td>5.481297</td>\n      <td>75.441176</td>\n      <td>0.640898</td>\n      <td>514</td>\n      <td>5197</td>\n      <td>0</td>\n      <td>0.514366</td>\n      <td>0.073566</td>\n      <td>0.112219</td>\n      <td>...</td>\n      <td>0.059938</td>\n      <td>-0.004517</td>\n      <td>0.064531</td>\n      <td>0.129023</td>\n      <td>-0.062811</td>\n      <td>0.001734</td>\n      <td>-0.053279</td>\n      <td>-0.044899</td>\n      <td>0.018924</td>\n      <td>Western</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t2_3edl7</td>\n      <td>5.916865</td>\n      <td>60.968085</td>\n      <td>0.719715</td>\n      <td>606</td>\n      <td>5823</td>\n      <td>11</td>\n      <td>0.401854</td>\n      <td>0.122328</td>\n      <td>0.085511</td>\n      <td>...</td>\n      <td>-0.016096</td>\n      <td>-0.002657</td>\n      <td>0.096512</td>\n      <td>0.061793</td>\n      <td>-0.128142</td>\n      <td>0.001301</td>\n      <td>-0.059142</td>\n      <td>0.026707</td>\n      <td>-0.030807</td>\n      <td>Western</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30934</th>\n      <td>t2_8hacr7if</td>\n      <td>5.697468</td>\n      <td>49.875000</td>\n      <td>0.683544</td>\n      <td>540</td>\n      <td>5290</td>\n      <td>13</td>\n      <td>0.437244</td>\n      <td>0.111392</td>\n      <td>0.037975</td>\n      <td>...</td>\n      <td>-0.054445</td>\n      <td>-0.005667</td>\n      <td>0.073418</td>\n      <td>0.111456</td>\n      <td>-0.142758</td>\n      <td>-0.026188</td>\n      <td>-0.088024</td>\n      <td>0.020324</td>\n      <td>-0.023312</td>\n      <td>Eastern</td>\n    </tr>\n    <tr>\n      <th>30935</th>\n      <td>t2_8hacr7if</td>\n      <td>5.790725</td>\n      <td>50.450450</td>\n      <td>0.744352</td>\n      <td>626</td>\n      <td>5710</td>\n      <td>9</td>\n      <td>0.436489</td>\n      <td>0.109394</td>\n      <td>0.048751</td>\n      <td>...</td>\n      <td>-0.053965</td>\n      <td>0.010229</td>\n      <td>0.090820</td>\n      <td>0.098168</td>\n      <td>-0.123997</td>\n      <td>0.001711</td>\n      <td>-0.086069</td>\n      <td>0.063151</td>\n      <td>-0.029690</td>\n      <td>Eastern</td>\n    </tr>\n    <tr>\n      <th>30936</th>\n      <td>t2_8hacr7if</td>\n      <td>5.671835</td>\n      <td>47.716981</td>\n      <td>0.700258</td>\n      <td>542</td>\n      <td>5163</td>\n      <td>9</td>\n      <td>0.466545</td>\n      <td>0.131783</td>\n      <td>0.047804</td>\n      <td>...</td>\n      <td>0.012857</td>\n      <td>-0.056423</td>\n      <td>0.078301</td>\n      <td>0.113703</td>\n      <td>-0.119587</td>\n      <td>-0.012560</td>\n      <td>-0.153321</td>\n      <td>0.051385</td>\n      <td>-0.040174</td>\n      <td>Eastern</td>\n    </tr>\n    <tr>\n      <th>30937</th>\n      <td>t2_8hacr7if</td>\n      <td>5.748459</td>\n      <td>47.866071</td>\n      <td>0.704069</td>\n      <td>571</td>\n      <td>5472</td>\n      <td>7</td>\n      <td>0.428413</td>\n      <td>0.114673</td>\n      <td>0.053021</td>\n      <td>...</td>\n      <td>0.011493</td>\n      <td>-0.041948</td>\n      <td>0.091651</td>\n      <td>0.121716</td>\n      <td>-0.118058</td>\n      <td>-0.020970</td>\n      <td>-0.146114</td>\n      <td>0.043594</td>\n      <td>-0.022459</td>\n      <td>Eastern</td>\n    </tr>\n    <tr>\n      <th>30938</th>\n      <td>t2_8hacr7if</td>\n      <td>5.859155</td>\n      <td>46.161290</td>\n      <td>0.892019</td>\n      <td>190</td>\n      <td>1460</td>\n      <td>6</td>\n      <td>0.425926</td>\n      <td>0.107981</td>\n      <td>0.032864</td>\n      <td>...</td>\n      <td>-0.001145</td>\n      <td>0.023824</td>\n      <td>0.112719</td>\n      <td>0.103603</td>\n      <td>-0.121641</td>\n      <td>-0.011123</td>\n      <td>-0.093972</td>\n      <td>0.082738</td>\n      <td>-0.009133</td>\n      <td>Eastern</td>\n    </tr>\n  </tbody>\n</table>\n<p>30939 rows × 113 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/Word2Vec_feature_data.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression + Evaluation metrics function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def Log_Reg(df, embeddings, X):\n",
    "    \"\"\"\n",
    "    Function used to run logistic regression and print several evaluation metrics such as: precision, recall, accuracy, F1 score across classes as well as a Classification report. Function also prints the 5-fold cross validation scores for precision.\n",
    "    Input parameter descriptions:\n",
    "    new_df : the input dataframe with required features as well as outcome variable for model (type DataFrame)\n",
    "    embeddings : boolean value, used to run either the baseline model without embeddings or the model with embeddings (type Boolean)\n",
    "    X : the input dataframe either with or wthout document embeddings (type DataFrame)\n",
    "    \"\"\"\n",
    "    # y data\n",
    "    y = df['Poles']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the logistic regression model\n",
    "    clf = LogisticRegression(penalty='l2', C=1.0, max_iter=10000)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='weighted')\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "    mcc = matthews_corrcoef(y_test, predictions)\n",
    "    if embeddings:\n",
    "        print(\"Evaluation metrics of Logistic Regression WITH embeddings are as follows: \")\n",
    "    else:\n",
    "        print(\"Evaluation metrics of Baseline Logistic Regression (WITHOUT embeddings) are as follows: \")\n",
    "    print()\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1-score: {f1}')\n",
    "    print(f'Matthews Correlation Coefficient: {mcc}')\n",
    "    print()\n",
    "\n",
    "    # Generate a classification report\n",
    "    class_report = classification_report(y_test, predictions)\n",
    "    print('Classification Report:\\n', class_report)\n",
    "    print()\n",
    "\n",
    "    # Perform 5-fold cross-validation\n",
    "    cv_scores = cross_val_score(clf, X, y, cv=5, scoring='f1_weighted')\n",
    "    # Print cross-validation scores\n",
    "    print('Cross-Validation Scores:', cv_scores)\n",
    "    print('Mean CV Accuracy:', np.mean(cv_scores))\n",
    "\n",
    "    # returning the model\n",
    "    return clf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Selection (User input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our baseline model is a simple logistic regression model without document embeddings but only linguistic and grammatical features such as average sentence length, subjectivity, etc. We also decided to test the performance of the same model but this time with the document embeddings as additional features.\n",
    "\n",
    "When prompted with making an input, enter \"yes\" if you want to obtain the results of the logistic regression with the document embeddings and enter \"no\", if you would like to obtain the results of the baseline model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have entered a valid input\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\" Do you want to test the logistic regression with document embeddings (yes/no)?\")\n",
    "\n",
    "    if user_input.lower() == \"yes\":\n",
    "        embeddings = True\n",
    "        # Creating X dataframe with document embeddings\n",
    "        X = df.drop(['author_ID', 'Poles', ], axis=1)\n",
    "        break\n",
    "    elif user_input.lower() == \"no\":\n",
    "        embeddings = False\n",
    "        # Creating X dataframe by extracting all features except document eembeddings\n",
    "        X = df.loc[:, ['average_word_length','avg_sentence_length', 'ttr', 'nr_unique_words', 'nr_chars', 'nr_contradictions', 'subjectivity','nr_period', 'nr_comma', 'nr_question', 'nr_exclamation']]\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid input. Please enter 'yes' or 'no'. Try again.\")\n",
    "\n",
    "print()\n",
    "print(\"You have entered a valid input\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Displaying Evaluation Metrics of chosen model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics of Logistic Regression WITH embeddings are as follows: \n",
      "\n",
      "Accuracy: 0.8055914673561733\n",
      "Precision: 0.8056890113280266\n",
      "Recall: 0.8055914673561733\n",
      "F1-score: 0.805597636135838\n",
      "Matthews Correlation Coefficient: 0.6112607449669624\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Eastern       0.80      0.81      0.80      3058\n",
      "     Western       0.81      0.80      0.81      3130\n",
      "\n",
      "    accuracy                           0.81      6188\n",
      "   macro avg       0.81      0.81      0.81      6188\n",
      "weighted avg       0.81      0.81      0.81      6188\n",
      "\n",
      "\n",
      "Cross-Validation Scores: [0.81281167 0.77659507 0.70505122 0.61198106 0.7307442 ]\n",
      "Mean CV Accuracy: 0.7274366452006724\n"
     ]
    }
   ],
   "source": [
    "model = Log_Reg(df, embeddings, X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Importance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "                Feature  Coefficient  Absolute_Coefficient\n42           feature_31    -7.445960              7.445960\n30           feature_19     7.103592              7.103592\n60           feature_49    -7.077410              7.077410\n52           feature_41     6.974895              6.974895\n77           feature_66     6.583141              6.583141\n..                  ...          ...                   ...\n5     nr_contradictions     0.025130              0.025130\n90           feature_79    -0.013925              0.013925\n1   avg_sentence_length     0.003166              0.003166\n3       nr_unique_words    -0.002646              0.002646\n4              nr_chars    -0.000288              0.000288\n\n[111 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Coefficient</th>\n      <th>Absolute_Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42</th>\n      <td>feature_31</td>\n      <td>-7.445960</td>\n      <td>7.445960</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>feature_19</td>\n      <td>7.103592</td>\n      <td>7.103592</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>feature_49</td>\n      <td>-7.077410</td>\n      <td>7.077410</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>feature_41</td>\n      <td>6.974895</td>\n      <td>6.974895</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>feature_66</td>\n      <td>6.583141</td>\n      <td>6.583141</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>nr_contradictions</td>\n      <td>0.025130</td>\n      <td>0.025130</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>feature_79</td>\n      <td>-0.013925</td>\n      <td>0.013925</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>avg_sentence_length</td>\n      <td>0.003166</td>\n      <td>0.003166</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>nr_unique_words</td>\n      <td>-0.002646</td>\n      <td>0.002646</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>nr_chars</td>\n      <td>-0.000288</td>\n      <td>0.000288</td>\n    </tr>\n  </tbody>\n</table>\n<p>111 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the model has the attribute coef_ (handles regularization)\n",
    "if hasattr(model, 'coef_'):\n",
    "    coefficients = model.coef_[0]\n",
    "else:\n",
    "    # For models with regularization, coefficients are stored in coef_ only if not regularized\n",
    "    coefficients = model.coef_\n",
    "\n",
    "# Create a DataFrame to display feature names and their corresponding coefficients\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort features by absolute coefficient values (importance)\n",
    "feature_importance_df['Absolute_Coefficient'] = feature_importance_df['Coefficient'].abs()\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Absolute_Coefficient', ascending=False)\n",
    "\n",
    "# Display the feature importance DataFrame\n",
    "feature_importance_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature ranking of linguistic + grammatical features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We rank the features based on how important it is for the binary classifcation task of the model. Features higher up in ranking have higher absolute coefficient values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 49: ['nr_comma', -2.2895543034254904, 2.2895543034254904]\n",
      "Row 53: ['subjectivity', -1.9952040714440074, 1.9952040714440074]\n",
      "Row 75: ['nr_period', -1.162971792104766, 1.162971792104766]\n",
      "Row 82: ['average_word_length', 0.6951716306608383, 0.6951716306608383]\n",
      "Row 94: ['ttr', -0.29434231416973594, 0.29434231416973594]\n",
      "Row 95: ['nr_exclamation', 0.28701172063714137, 0.28701172063714137]\n",
      "Row 105: ['nr_question', -0.044363594028351994, 0.044363594028351994]\n",
      "Row 107: ['nr_contradictions', 0.025130474545760444, 0.025130474545760444]\n",
      "Row 109: ['avg_sentence_length', 0.003165755094896711, 0.003165755094896711]\n",
      "Row 110: ['nr_unique_words', -0.002645644568672686, 0.002645644568672686]\n",
      "Row 111: ['nr_chars', -0.0002876709713411503, 0.0002876709713411503]\n"
     ]
    }
   ],
   "source": [
    "# re-setting the index to make row numbers easily readable\n",
    "feature_importance_df = feature_importance_df.reset_index(drop=True)\n",
    "\n",
    "values_to_find = ['average_word_length', 'avg_sentence_length', 'ttr', 'nr_unique_words', 'nr_chars', 'nr_contradictions', 'subjectivity', 'nr_period', 'nr_comma', 'nr_question', 'nr_exclamation']\n",
    "\n",
    "# Create a boolean mask for each value in the list\n",
    "mask = feature_importance_df.isin(values_to_find)\n",
    "\n",
    "# Use the any() method to check if any of the values match in each row\n",
    "result = mask.any(axis=1)\n",
    "\n",
    "# Get the row numbers where the values are present\n",
    "row_numbers = result.index[result].tolist()\n",
    "\n",
    "# Print row numbers and respective values\n",
    "for row_number in row_numbers:\n",
    "    values = feature_importance_df.iloc[row_number].values.tolist()\n",
    "    print(f\"Row {row_number + 1}: {values}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}