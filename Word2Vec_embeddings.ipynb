{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import ast"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-trained GloVe vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember to download the pre-trained GloVe vectors (Twitter database) from the following link and use the file with 100 dimensions (100d) and then create and store it in the \"Data\" folder : https://nlp.stanford.edu/projects/glove/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20211147\\AppData\\Local\\Temp\\ipykernel_6164\\2796374558.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  _ = glove2word2vec(glove_file, tmp_file)\n"
     ]
    }
   ],
   "source": [
    "glove_file = datapath('Data/glove.twitter.27B.100d.txt')\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "glove_vectors = KeyedVectors.load_word2vec_format(tmp_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-2-vec model (basic implementation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Loading the entire dataset for creating the word embeddings\n",
    "df = pd.read_csv(\"Data/tokenized_eng.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Extracting only the token column as a pandas series\n",
    "documents = df['eng_tokens']\n",
    "\n",
    "# Convert the string representations of lists to actual lists using ast.literal_eval\n",
    "sentences = [ast.literal_eval(sentence) for sentence in documents]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# build a word2vec model on your dataset\n",
    "base_model = Word2Vec(vector_size=100, min_count=10, sg=1) # skip gram model\n",
    "base_model.build_vocab(sentences)\n",
    "total_examples = base_model.corpus_count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding GloVe weights and retraining"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Update the vocabulary of the base_model with the keys from glove_vectors\n",
    "base_model.build_vocab([list(glove_vectors.index_to_key)], update=True)\n",
    "\n",
    "# train on your data\n",
    "base_model.train(sentences, total_examples=total_examples, epochs=base_model.epochs)\n",
    "base_model_wv = base_model.wv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Feature matrix (with document embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Loading the balanced dataset\n",
    "df_balanced = pd.read_csv(\"Data/Undersampled_balanced_data.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "         author_ID average_word_length avg_sentence_length   \n0      t2_ffcfiueh   5.481442205726405   66.16483516483517  \\\n1         t2_lfs48   6.285714285714286   85.01492537313433   \n2         t2_zcj4y   5.580459770114943   34.13496932515337   \n3      t2_2xpu7n1c  5.4812967581047385   75.44117647058823   \n4         t2_3edl7  5.9168646080760094   60.96808510638298   \n...            ...                 ...                 ...   \n30937  t2_8hacr7if    5.69746835443038              49.875   \n30938  t2_8hacr7if  5.7907253269916765   50.45045045045045   \n30939  t2_8hacr7if   5.671834625322997   47.71698113207548   \n30940  t2_8hacr7if    5.74845869297164   47.86607142857143   \n30941  t2_8hacr7if   5.859154929577465   46.16129032258065   \n\n                      ttr nr_unique_words nr_chars nr_contradictions   \n0      0.7073170731707317             667     6111                 6  \\\n1       0.718078381795196             568     5762                17   \n2      0.7471264367816092             650     5724                18   \n3      0.6408977556109726             514     5197                 0   \n4      0.7197149643705463             606     5823                11   \n...                   ...             ...      ...               ...   \n30937  0.6835443037974683             540     5290                13   \n30938  0.7443519619500595             626     5710                 9   \n30939  0.7002583979328165             542     5163                 9   \n30940  0.7040690505548706             571     5472                 7   \n30941   0.892018779342723             190     1460                 6   \n\n             subjectivity           nr_period            nr_comma  ...   \n0      0.4854949874686716  0.0911983032873807  0.0477200424178154  ...  \\\n1       0.487030497888162    0.11378002528445  0.0783817951959544  ...   \n2      0.5510049760644998  0.1747126436781609  0.0655172413793103  ...   \n3         0.5143662238145  0.0735660847880299   0.112219451371571  ...   \n4      0.4018541930046354  0.1223277909738717  0.0855106888361045  ...   \n...                   ...                 ...                 ...  ...   \n30937  0.4372444684944683  0.1113924050632911  0.0379746835443038  ...   \n30938  0.4364888583638583  0.1093935790725326  0.0487514863258026  ...   \n30939  0.4665453055398707  0.1317829457364341  0.0478036175710594  ...   \n30940  0.4284130624726955  0.1146732429099876  0.0530209617755857  ...   \n30941  0.4259259259259259   0.107981220657277  0.0328638497652582  ...   \n\n      feature_91 feature_92  feature_93  feature_94  feature_95  feature_96   \n0      -0.007784   0.031585    0.049738    0.100031   -0.031415   -0.071001  \\\n1       0.032240   0.023061    0.054812    0.041297   -0.027160   -0.021973   \n2       0.011856  -0.051878    0.068134    0.081694    0.018575   -0.040013   \n3       0.018508  -0.034094    0.069904    0.126776    0.027026   -0.037627   \n4      -0.009086   0.012244    0.106775    0.108287   -0.079001   -0.050491   \n...          ...        ...         ...         ...         ...         ...   \n30937   0.000702  -0.014954    0.096263    0.116204   -0.052225   -0.058670   \n30938  -0.001142   0.008996    0.093147    0.125046   -0.044154   -0.022087   \n30939  -0.006764  -0.022218    0.129190    0.094650   -0.095174   -0.005999   \n30940   0.003105  -0.023637    0.137076    0.091326   -0.080962   -0.032644   \n30941  -0.000619  -0.000171    0.125674    0.102669   -0.066256   -0.052388   \n\n       feature_97  feature_98  feature_99    Poles  \n0        0.038708    0.066147   -0.077816  Western  \n1        0.006359    0.030645   -0.081342  Western  \n2        0.047502    0.069519   -0.064935  Western  \n3        0.040436    0.025213   -0.070703  Western  \n4        0.046639    0.056835   -0.030492  Western  \n...           ...         ...         ...      ...  \n30937    0.011701    0.083648    0.000851  Eastern  \n30938    0.003849    0.087167   -0.020898  Eastern  \n30939   -0.066520    0.104894    0.030986  Eastern  \n30940   -0.063697    0.111811    0.031217  Eastern  \n30941   -0.011592    0.096296   -0.008148  Eastern  \n\n[30942 rows x 113 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author_ID</th>\n      <th>average_word_length</th>\n      <th>avg_sentence_length</th>\n      <th>ttr</th>\n      <th>nr_unique_words</th>\n      <th>nr_chars</th>\n      <th>nr_contradictions</th>\n      <th>subjectivity</th>\n      <th>nr_period</th>\n      <th>nr_comma</th>\n      <th>...</th>\n      <th>feature_91</th>\n      <th>feature_92</th>\n      <th>feature_93</th>\n      <th>feature_94</th>\n      <th>feature_95</th>\n      <th>feature_96</th>\n      <th>feature_97</th>\n      <th>feature_98</th>\n      <th>feature_99</th>\n      <th>Poles</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t2_ffcfiueh</td>\n      <td>5.481442205726405</td>\n      <td>66.16483516483517</td>\n      <td>0.7073170731707317</td>\n      <td>667</td>\n      <td>6111</td>\n      <td>6</td>\n      <td>0.4854949874686716</td>\n      <td>0.0911983032873807</td>\n      <td>0.0477200424178154</td>\n      <td>...</td>\n      <td>-0.007784</td>\n      <td>0.031585</td>\n      <td>0.049738</td>\n      <td>0.100031</td>\n      <td>-0.031415</td>\n      <td>-0.071001</td>\n      <td>0.038708</td>\n      <td>0.066147</td>\n      <td>-0.077816</td>\n      <td>Western</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t2_lfs48</td>\n      <td>6.285714285714286</td>\n      <td>85.01492537313433</td>\n      <td>0.718078381795196</td>\n      <td>568</td>\n      <td>5762</td>\n      <td>17</td>\n      <td>0.487030497888162</td>\n      <td>0.11378002528445</td>\n      <td>0.0783817951959544</td>\n      <td>...</td>\n      <td>0.032240</td>\n      <td>0.023061</td>\n      <td>0.054812</td>\n      <td>0.041297</td>\n      <td>-0.027160</td>\n      <td>-0.021973</td>\n      <td>0.006359</td>\n      <td>0.030645</td>\n      <td>-0.081342</td>\n      <td>Western</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t2_zcj4y</td>\n      <td>5.580459770114943</td>\n      <td>34.13496932515337</td>\n      <td>0.7471264367816092</td>\n      <td>650</td>\n      <td>5724</td>\n      <td>18</td>\n      <td>0.5510049760644998</td>\n      <td>0.1747126436781609</td>\n      <td>0.0655172413793103</td>\n      <td>...</td>\n      <td>0.011856</td>\n      <td>-0.051878</td>\n      <td>0.068134</td>\n      <td>0.081694</td>\n      <td>0.018575</td>\n      <td>-0.040013</td>\n      <td>0.047502</td>\n      <td>0.069519</td>\n      <td>-0.064935</td>\n      <td>Western</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t2_2xpu7n1c</td>\n      <td>5.4812967581047385</td>\n      <td>75.44117647058823</td>\n      <td>0.6408977556109726</td>\n      <td>514</td>\n      <td>5197</td>\n      <td>0</td>\n      <td>0.5143662238145</td>\n      <td>0.0735660847880299</td>\n      <td>0.112219451371571</td>\n      <td>...</td>\n      <td>0.018508</td>\n      <td>-0.034094</td>\n      <td>0.069904</td>\n      <td>0.126776</td>\n      <td>0.027026</td>\n      <td>-0.037627</td>\n      <td>0.040436</td>\n      <td>0.025213</td>\n      <td>-0.070703</td>\n      <td>Western</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t2_3edl7</td>\n      <td>5.9168646080760094</td>\n      <td>60.96808510638298</td>\n      <td>0.7197149643705463</td>\n      <td>606</td>\n      <td>5823</td>\n      <td>11</td>\n      <td>0.4018541930046354</td>\n      <td>0.1223277909738717</td>\n      <td>0.0855106888361045</td>\n      <td>...</td>\n      <td>-0.009086</td>\n      <td>0.012244</td>\n      <td>0.106775</td>\n      <td>0.108287</td>\n      <td>-0.079001</td>\n      <td>-0.050491</td>\n      <td>0.046639</td>\n      <td>0.056835</td>\n      <td>-0.030492</td>\n      <td>Western</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30937</th>\n      <td>t2_8hacr7if</td>\n      <td>5.69746835443038</td>\n      <td>49.875</td>\n      <td>0.6835443037974683</td>\n      <td>540</td>\n      <td>5290</td>\n      <td>13</td>\n      <td>0.4372444684944683</td>\n      <td>0.1113924050632911</td>\n      <td>0.0379746835443038</td>\n      <td>...</td>\n      <td>0.000702</td>\n      <td>-0.014954</td>\n      <td>0.096263</td>\n      <td>0.116204</td>\n      <td>-0.052225</td>\n      <td>-0.058670</td>\n      <td>0.011701</td>\n      <td>0.083648</td>\n      <td>0.000851</td>\n      <td>Eastern</td>\n    </tr>\n    <tr>\n      <th>30938</th>\n      <td>t2_8hacr7if</td>\n      <td>5.7907253269916765</td>\n      <td>50.45045045045045</td>\n      <td>0.7443519619500595</td>\n      <td>626</td>\n      <td>5710</td>\n      <td>9</td>\n      <td>0.4364888583638583</td>\n      <td>0.1093935790725326</td>\n      <td>0.0487514863258026</td>\n      <td>...</td>\n      <td>-0.001142</td>\n      <td>0.008996</td>\n      <td>0.093147</td>\n      <td>0.125046</td>\n      <td>-0.044154</td>\n      <td>-0.022087</td>\n      <td>0.003849</td>\n      <td>0.087167</td>\n      <td>-0.020898</td>\n      <td>Eastern</td>\n    </tr>\n    <tr>\n      <th>30939</th>\n      <td>t2_8hacr7if</td>\n      <td>5.671834625322997</td>\n      <td>47.71698113207548</td>\n      <td>0.7002583979328165</td>\n      <td>542</td>\n      <td>5163</td>\n      <td>9</td>\n      <td>0.4665453055398707</td>\n      <td>0.1317829457364341</td>\n      <td>0.0478036175710594</td>\n      <td>...</td>\n      <td>-0.006764</td>\n      <td>-0.022218</td>\n      <td>0.129190</td>\n      <td>0.094650</td>\n      <td>-0.095174</td>\n      <td>-0.005999</td>\n      <td>-0.066520</td>\n      <td>0.104894</td>\n      <td>0.030986</td>\n      <td>Eastern</td>\n    </tr>\n    <tr>\n      <th>30940</th>\n      <td>t2_8hacr7if</td>\n      <td>5.74845869297164</td>\n      <td>47.86607142857143</td>\n      <td>0.7040690505548706</td>\n      <td>571</td>\n      <td>5472</td>\n      <td>7</td>\n      <td>0.4284130624726955</td>\n      <td>0.1146732429099876</td>\n      <td>0.0530209617755857</td>\n      <td>...</td>\n      <td>0.003105</td>\n      <td>-0.023637</td>\n      <td>0.137076</td>\n      <td>0.091326</td>\n      <td>-0.080962</td>\n      <td>-0.032644</td>\n      <td>-0.063697</td>\n      <td>0.111811</td>\n      <td>0.031217</td>\n      <td>Eastern</td>\n    </tr>\n    <tr>\n      <th>30941</th>\n      <td>t2_8hacr7if</td>\n      <td>5.859154929577465</td>\n      <td>46.16129032258065</td>\n      <td>0.892018779342723</td>\n      <td>190</td>\n      <td>1460</td>\n      <td>6</td>\n      <td>0.4259259259259259</td>\n      <td>0.107981220657277</td>\n      <td>0.0328638497652582</td>\n      <td>...</td>\n      <td>-0.000619</td>\n      <td>-0.000171</td>\n      <td>0.125674</td>\n      <td>0.102669</td>\n      <td>-0.066256</td>\n      <td>-0.052388</td>\n      <td>-0.011592</td>\n      <td>0.096296</td>\n      <td>-0.008148</td>\n      <td>Eastern</td>\n    </tr>\n  </tbody>\n</table>\n<p>30942 rows × 113 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize empty lists to store data\n",
    "author_ids = []\n",
    "avg_word_length = []\n",
    "avg_sent_length = []\n",
    "ttr = []\n",
    "nr_unique_words = []\n",
    "nr_chars = []\n",
    "nr_contradictions = []\n",
    "subjectivity = []\n",
    "feature_vectors = []\n",
    "labels = []\n",
    "nr_period = []\n",
    "nr_comma = []\n",
    "nr_question = []\n",
    "nr_exclamation = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df_balanced.iterrows():\n",
    "    sentence = ast.literal_eval(row['eng_tokens'])\n",
    "    # Get the sentence vector by taking average of all word embeddings in sentence\n",
    "    word_embeddings = [base_model_wv[token] for token in sentence if token in base_model_wv]\n",
    "\n",
    "    # Ensure all word embeddings have the same length\n",
    "    # Sometimes they have varying lengths because not all words are embedded in Word2Vec\n",
    "    # LIMITATION of Word2Vec!!!\n",
    "    if word_embeddings:\n",
    "        sentence_vector = np.mean(word_embeddings, axis=0)\n",
    "        feature_vectors.append(sentence_vector)\n",
    "    else:\n",
    "        # Handle the case where no embeddings are available\n",
    "        feature_vectors.append(np.zeros(base_model_wv.vector_size))\n",
    "\n",
    "    # Append author_id, feature_vector, and label, etc. to respective lists\n",
    "    author_ids.append(row['auhtor_ID'])\n",
    "    avg_word_length.append(row['average_word_length'])\n",
    "    avg_sent_length.append(row['avg_sentence_length'])\n",
    "    ttr.append(row['ttr'])\n",
    "    nr_unique_words.append(row['nr_unique_words'])\n",
    "    nr_chars.append(row['nr_chars'])\n",
    "    nr_contradictions.append(row['nr_contradictions'])\n",
    "    subjectivity.append(row['subjectivity'])\n",
    "    nr_period.append(row['norm_.'])\n",
    "    nr_comma.append(row['norm_,'])\n",
    "    nr_question.append(row['norm_?'])\n",
    "    nr_exclamation.append(row['norm_!'])\n",
    "\n",
    "    labels.append(row['Poles'])\n",
    "\n",
    "# Create a new DataFrame with author IDs, feature vectors, and labels, etc.\n",
    "columns = [f'feature_{i}' for i in range(len(feature_vectors[0]))]\n",
    "additional_columns = ['average_word_length', 'avg_sentence_length', 'ttr', 'nr_unique_words', 'nr_chars', 'nr_contradictions', 'subjectivity','nr_period', 'nr_comma', 'nr_question', 'nr_exclamation']\n",
    "data = np.concatenate([np.array(author_ids).reshape(-1, 1), np.array(avg_word_length).reshape(-1, 1),\n",
    "                       np.array(avg_sent_length).reshape(-1, 1), np.array(ttr).reshape(-1, 1),\n",
    "                       np.array(nr_unique_words).reshape(-1, 1), np.array(nr_chars).reshape(-1, 1),\n",
    "                       np.array(nr_contradictions).reshape(-1, 1), np.array(subjectivity).reshape(-1, 1),\n",
    "                       np.array(nr_period).reshape(-1, 1), np.array(nr_comma).reshape(-1, 1),\n",
    "                       np.array(nr_question).reshape(-1, 1), np.array(nr_exclamation).reshape(-1, 1),\n",
    "                       np.array(feature_vectors).tolist(), np.array(labels).reshape(-1, 1)], axis=1)\n",
    "\n",
    "new_df = pd.DataFrame(data, columns=['author_ID'] + additional_columns + columns + ['Poles'])\n",
    "\n",
    "# Convert feature columns to numeric\n",
    "new_df[columns] = new_df[columns].apply(pd.to_numeric)\n",
    "new_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "## Removing all the rows with 0 values (OOV words not captured!!)\n",
    "new_df = new_df[new_df['feature_0'] != 0.0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "## Converting df into csv file\n",
    "new_df.to_csv('Data/Word2Vec_feature_data.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}